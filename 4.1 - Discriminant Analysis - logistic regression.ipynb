{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Logistic Regression\n",
    "\n",
    "---\n",
    "> Erick Eduardo Aguilar Hernández:\n",
    "> * mat.ErickAguilar@gmail.com.mx\n",
    "> * isc.ErickAguilar@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider two populations $\\Omega_1$, $\\Omega_2$ with their random vectors of features associated $\\mathbf{x_1},\\mathbf{x_2} \\in \\mathbb{R}^p$, a discriminant is a function $D:\\mathbb{R}^p \\rightarrow \\mathbb{R}$ such that if $D(\\mathbf{x}) \\geq t$ for some threshold $t \\in \\mathbb{R}$ then asign $\\mathbf{x}$ to $\\Omega_1$ otherwise $\\mathbf{x}$ to $\\Omega_2$. D its a linear discriminant if D has the form:\n",
    "\n",
    "$$D(\\mathbf{x})=\\beta_0+\\beta_1 x_1 + \\dots \\beta_p x_p = \\beta_0+\\mathbf{x' \\beta}$$\n",
    "\n",
    "### Logistic Model\n",
    "___\n",
    "\n",
    "Consider the following random vector $(Y,X_1,\\dots,Y_p) \\in {0,1} \\times \\mathbb{R}^p$, where $Y$ is and indicator variable and this take the value 1 if the random vector belong to the population $\\Omega_1$ and 0 if it belongs to $\\Omega_2$, so it make sense that the probability of belonging to $\\Omega_1$ (objetive population) sould be parametrized by a discriminant D, where D its a linear combination of features and weights:\n",
    "\n",
    "$$P[Y=Y|X_1=x_1,\\dots,X_p=x_p]=p(D)=p(\\mathbf{x};\\beta_0,\\mathbf{\\beta})$$\n",
    "\n",
    "The logistic model establishes that the increase in the probability of belonging to the population $\\Omega_1$ with respect to discriminant D is directly proportional to the probability of belonging to $\\Omega_1$ given D and decreases as it approach to 1. \n",
    "\n",
    "$$ \\frac{p'}{p}=1-p$$\n",
    "\n",
    "Tha idea its try to solve this differential equation in order to put $p$ as function of the values of D.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{p'}{p} &= 1-p\\\\\n",
    "p' &= p(1-p)\\\\\n",
    "\\frac{dp}{dD} &= p(1-p)\\\\\n",
    "\\frac{dp}{p(1-p)}&=dD\\\\\n",
    "\\int \\frac{dp}{p(1-p)}&=\\int dD\\\\\n",
    "\\int \\frac{dp}{p-p^2}&=D\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Note that the constant of the integral in the right side couuld be absorved by the intercepto of D, and the only problem its try to solve the left side integral. Using partial fractions:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{1}{p(1-p)} &= \\frac{1}{p}+\\frac{1}{1-p} \\\\\n",
    "1 &= \\frac{A}{p}(p(1-p))+\\frac{B}{1-p}(p(1-p)) \\\\\n",
    "1 &= A(1-p)+Bp\\\\\n",
    "& \\left \\{ \\begin{matrix} - B &= -1\n",
    "\\\\ A+B &= 0 \\end{matrix}\\right.\\\\\n",
    "& A = 1, B = -1 \\\\\n",
    "\\implies \\frac{1}{p-p^2} &= \\frac{1}{p} - \\frac{1}{1-p} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Returning to the solution of the differential equation:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\int \\frac{dp}{p-p^2} &= D \\\\\n",
    "\\int \\frac{1}{p}dp - \\int \\frac{1}{1-p}dp &= D\\\\\n",
    "ln(p)-ln(1-p) &= D\\\\\n",
    "ln\\left(\\frac{p}{1-p}\\right) &= D  \\dots (1)\\\\\n",
    "\\frac{p}{1-p} &= e^D\\\\\n",
    "p &= (1-p) e^D\\\\\n",
    "p &= e^D-pe^D\\\\\n",
    "p + pe^D &= e^D\\\\\n",
    "p(1+e^D) &= e^D\\\\\n",
    "p &= \\frac{e^D}{1+e^D}\\\\\n",
    "p &= \\frac{1}{1+e^D}\\frac{1}{e^{-D}}\\\\\n",
    "p &= \\frac{1}{1+e^{-D}}\\\\\n",
    "p(\\mathbf{x}) &= \\frac{1}{1+e^{-(\\beta_0+\\mathbf{x' \\beta}})} ... (2)\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "From **equation (1)** the left side is know as **logit transformation**, and is a way to estimate a proportion to belong or nor to the objetive population. \n",
    "\n",
    "$$ ln\\left(\\frac{p(\\mathbf{x})}{1-p(\\mathbf{x})}\\right) = \\beta_0+\\beta_1 x_1 + \\dots \\beta_p x_p$$\n",
    "\n",
    "The transformation gives a bijection from the probability scale $\\left[0,1\\right]$ to the logit scale $\\left[−\\infty,\\infty\\right]$. **The equation (2)** gives the probability to belong to $\\Omega_1$ as function of the features of weights and the features of $\\Omega_1$, the argument of logarithm $\\frac{p}{1-p}$ its called the odds ratio. This equations its called the **logistic equation or sigmoid**. \n",
    "\n",
    "$$p(\\mathbf{x}) = \\frac{1}{1+e^{-(\\beta_0+\\beta_1 x_1 + \\dots \\beta_p x_p)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "___\n",
    "\n",
    "Since the indicator variable follows a Bernoulli distribution the Likehood function asociated to $p(\\mathbf{x})$ for the data matrix $\\mathbf{X} \\in M_{n \\times p}(\\mathbb{R})$, where each row are indepent observation without being identically distributed in general:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\beta_0,\\beta_1,...,\\beta_p) = \\prod_{i=1}^{n} p(\\mathbf{x}_i)^{y_i}(p(\\mathbf{x}_i))^{1-y_i}\n",
    "$$\n",
    "\n",
    "The logarithm of the Likehood function is:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "ln \\mathcal{L}(\\beta_0,\\beta_1,...,\\beta_p) &= \\sum_{i=1}^{n} y_i ln \\left(p(\\mathbf{x}_i)\\right)+\\sum_{i=1}^{n} (1-y_i)ln \\left(1-p(\\mathbf{x}_i)\\right) \\\\\n",
    "& = \\sum_{i=1}^{n} y_i ln \\left(p(\\mathbf{x}_i)\\right)-\\sum_{i=1}^{n} y_i ln \\left(1-\n",
    "p(\\mathbf{x}_i)\\right)+\\sum_{i=1}^{n} ln \\left(1-p(\\mathbf{x}_i)\\right) \\\\\n",
    "& = \\sum_{i=1}^{n} y_i \\left[ ln \\left(p(\\mathbf{x}_i)\\right)- ln \\left(1-p(\\mathbf{x}_i)\\right)\\right]+\\sum_{i=1}^{n} ln \\left(1-\\frac{1}{1+e^{-(\\beta_0+\\mathbf{x_i' \\beta})}}\\right) \\\\\n",
    "& = \\sum_{i=1}^{n} y_i ln\\left(\\frac{p(\\mathbf{x_i})}{1-p(\\mathbf{x_i})}\\right) +\\sum_{i=1}^{n} ln \\left(\\frac{e^{-(\\beta_0+\\mathbf{x_i' \\beta})}}{1+e^{-(\\beta_0+\\mathbf{x_i' \\beta})}}\\right)\\\\\n",
    "ln \\mathcal{L}(\\beta_0,\\beta_1,...,\\beta_p) &= \\sum_{i=1}^{n} y_i \\left[\\beta_0+\\mathbf{x_i' \\beta}\\right] +\\sum_{i=1}^{n} ln \\left(\\frac{1}{1+e^{-(\\beta_0+\\mathbf{x_i' \\beta})}}\\right) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Computing the partials to obtain the logistic normal equations:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial ln \\mathcal{L}(\\beta_0,\\beta_1,...,\\beta_p)}{\\partial \\beta_0} &= \\sum_{i=1}^{n} y_i-\\sum_{i=1}^{n}\\frac{e^{\\beta_0+\\mathbf{x_i' \\beta}}}{1+e^{\\beta_0+\\mathbf{x_i' \\beta}}} = \\sum_{i=1}^{n} (y_i-p(\\mathbf{x_i})) = 0\\\\\n",
    "\\frac{\\partial ln \\mathcal{L}(\\beta_0,\\beta_1,...,\\beta_p)}{\\partial \\beta_j} &= \\sum_{i=1}^{n} x_{ij} y_i-\\sum_{i=1}^{n}x_{ij}\\frac{e^{\\beta_0+\\mathbf{x_i' \\beta}}}{1+e^{\\beta_0+\\mathbf{x_i' \\beta}}} = \\sum_{i=1}^{n} x_{ij} (y_i-p(\\mathbf{x_i})) = 0 \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "That equations are transcendental so they could not be solved in terms of elemental functions. This maximum likehood estimator should be computing using descent gradient.\n",
    "\n",
    "### Wald test for goodness of fit\n",
    "___\n",
    "Consider the MLE $\\hat{\\beta_j}$, by the asymptotic distribution theorem for the MLE, whe have that:\n",
    "\n",
    "$$\n",
    "\\frac{\\hat{\\beta_j}-B_0}{\\sqrt{V(\\hat{\\beta_j}})} \\sim N(0,1)\n",
    "\\implies \\frac{(\\hat{\\beta_j}-B_0)^2}{V(\\hat{\\beta_j})} \\sim \\chi_{(1)}\n",
    "$$\n",
    "\n",
    "Where $B_0$ is a fixed value to contrast, usually $B_0 = 0$ to test significance. Its possible to obtain the variance of the MLE vector $(\\hat{\\beta}_0,\\hat{\\beta}_1,...,\\hat{\\beta}_p)$ computing the Fisher information. $\\mathbf{I(\\beta)}=-E\\left[ \\nabla^2 ln \\mathcal{L}(\\beta_0,\\beta_1,...,\\beta_p) \\right]$, computing the partial derivates of second order:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2}{\\partial \\beta_j \\partial \\beta_k} \\mathcal{L}(\\beta_0,\\beta_1,...,\\beta_p) = -\\sum_{i=0}^n x_{ij}x_{ik} \\frac{e^{\\sum_{l=0}^{p} \\beta_l x_{il}}}{1+e^{\\sum_{l=0}^{p} \\beta_l x_{il}}}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

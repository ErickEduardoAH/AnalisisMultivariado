{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Support Vector Machine\n",
    "\n",
    "---\n",
    "> Erick Eduardo Aguilar Hernández:\n",
    "> * mat.ErickAguilar@ciencias.unam.mx\n",
    "> * isc.ErickAguilar@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suposse for the logistic regression model that given a fixed probability $p_0$ its necessary idenfy the regions in the feature space $p(\\mathbf{x};\\mathbf{\\beta}) > p_0$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p_0 &= \\frac{1}{1+e^{-(\\beta_0+\\beta_1 x_1 + \\dots + \\beta_p x_p)}} \\\\\n",
    "ln\\left(\\frac{ p_0}{1-p_0}\\right) & = \\beta_0+\\beta_1 x_1 + \\dots + \\beta_p x_p \\\\\n",
    "0 & = - ln\\left(\\frac{ p_0}{1-p_0}\\right) + \\beta_0 +\\beta_1 x_1 + \\dots + \\beta_p x_p\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Then the logistic model splits the feature space in two regions.\n",
    "\n",
    "$$\n",
    "P_{p_0} = \\{ \\mathbf{x} \\in \\mathbb{R}^p : - ln\\left(\\frac{ p_0}{1-p_0}\\right) + \\beta_0 +\\beta_1 x_1 + \\dots + \\beta_p x_p \\geq 0 \\} \\\\\n",
    "N_{p_0} =\\{ \\mathbf{x} \\in \\mathbb{R}^p : - ln\\left(\\frac{ p_0}{1-p_0}\\right) + \\beta_0 +\\beta_1 x_1 + \\dots + \\beta_p x_p < 0 \\}\n",
    "$$\n",
    "\n",
    "Particularly for $p_0$= 0.5 the regions are:\n",
    "\n",
    "$$P_{0.5} = \\{ \\mathbf{x} \\in \\mathbb{R}^p : \\beta_0 +\\beta_1 x_1 + \\dots + \\beta_p x_p \\geq 0 \\} \\\\\n",
    "N_{0.5} =\\{ \\mathbf{x} \\in \\mathbb{R}^p : \\beta_0 +\\beta_1 x_1 + \\dots + \\beta_p x_p < 0 \\}\n",
    "$$\n",
    "\n",
    "Although the logistic regression folds the feature space in to many different regions delimited by a decision boundary as hyperplanes, the logistic model dont provides the best threshold in order to find the best hyperplane that separates the populations. This motivates a new discriminant method known as suppor vector machine who principal objetive its try to determine the best hyperplane separator.\n",
    "\n",
    "\n",
    "### Geometric margins\n",
    "___\n",
    "\n",
    "Lets the hyperplane H with equation $w_1 x_1 + \\dots +w_p x_p + b = \\mathbf{w'x}+b$. Consider a point $\\mathbf{x} \\in H$ and other $\\mathbf{x_i} \\in \\mathbb{R}^p$ with label $y \\in \\{1,-1\\}$ (1 if $\\mathbf{x}$ belongs to positive class, -1 if $\\mathbf{x}$ belongs to negative class) then the distance between the point $\\mathbf{x_i}$ and H its given by:\n",
    "\n",
    "<img src='static/proyection.png' width=\"40%\" height=\"40%\">\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "d(\\mathbf{x_i},H) &= || Proy_\\mathbf{w} (\\mathbf{x}-\\mathbf{x_i}) || \\\\\n",
    " &= \\frac{< \\mathbf{w},(\\mathbf{x}-\\mathbf{x_i})>}{||\\mathbf{w}||} \\\\\n",
    " &= \\frac{w_1(x_{i1}-x_1)+\\dots+w_p(x_{ip}-xp)}{||\\mathbf{w}||} \\\\\n",
    " &= \\frac{w_1x_{i1}-w_1x_1+\\dots+w_px_{ip}-w_px_p}{||\\mathbf{w}||} \\\\\n",
    " &= \\frac{w_1x_{i1}+\\dots+w_px_{ip}-w_1x_1-\\dots-w_px_p}{||\\mathbf{w}||} \\\\\n",
    "&= \\frac{w_1x_{i1}+\\dots+w_px_{ip}+b}{||\\mathbf{w}||} \\\\\n",
    "d(\\mathbf{x_i},H)  &=\\frac{<\\mathbf{w},\\mathbf{x_i}>+b}{||\\mathbf{w}||}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Lets deffine the geometric margin for the ith observation $\\gamma_i$ as the oriented distance form the observation to the hyperplane such that if $\\mathbf{x_i}$ its a positive obervation then the orginal distance its conserved but if $\\mathbf{x_i}$ its a negative obervation then the distance have to change the sign to $-d(\\mathbf{x_i},H)$. since $y_i$ contains this information the equation for gamma its given by:\n",
    "\n",
    "<img src='static/margins.png' width=\"40%\" height=\"40%\">\n",
    "\n",
    "$$\\gamma_{\\mathbf{x_i}} (\\mathbf{w})= y_i \\left(\\frac{<\\mathbf{w},\\mathbf{x_i}>+b}{||\\mathbf{w}||}\\right) = y_i \\left(\\left(\\frac{\\mathbf{w'}}{||\\mathbf{w}||}\\right)\\mathbf{x_i} + \\frac{b}{||\\mathbf{w}||} \\right)$$\n",
    "\n",
    "\n",
    "### Optimal margin discriminant\n",
    "___\n",
    "\n",
    "The optimal hyperplane its given by the $\\mathbf{w} \\in \\mathbb{R}^p$ and $b \\in \\mathbb{R}$ such that:\n",
    "\n",
    "$$\n",
    "Max_{\\mathbf{w},b} \\left \\{ Min_{\\mathbf{x_i}} \\left \\{ \\left(\\frac{\\mathbf{w'}}{||\\mathbf{w}||}\\right)\\mathbf{x_i} + \\frac{b}{||\\mathbf{w}||} \\right \\} \\right \\} \\\\\n",
    "s.t. sign(\\gamma_{\\mathbf{x_i}})=y_i\n",
    "$$\n",
    "\n",
    "Solve this problemm in general its complicated due to:\n",
    "\n",
    "* The objective is nonlinear in two ways: the absolute value and the projection requires you to take a norm and * divide.\n",
    "* The constraints are nonlinear due to the sign comparisons.\n",
    "* There’s a min and a max! A priori, we have to do this because we don’t know which point is going to be the closest to the hyperplane.\n",
    "\n",
    "**Reformulating the problem**\n",
    "\n",
    "Consider 2 parallel hyperplanes $H_{1}$ and $H_{-1}$ with equations $w_1 x_1 + \\dots +w_p x_p + b = 1$ and $w_1 x_1 + \\dots +w_p x_p + b = -1$ conrrespondly and suposse there is not data observations between that planes.\n",
    "\n",
    "**Definition**: If $\\mathbf{z_0} \\in H_{-1}$, the margin is the perpendicular distance between $\\mathbf{z_0}$ and $H_1$ i.e. $m=d(\\mathbf{z_0},H_1)$.\n",
    "\n",
    "Contruct the following vector in same direction of $\\mathbf{w}$ but with lenght m, $\\mathbf{u} = m \\frac{\\mathbf{w}}{||\\mathbf{w}||}$, this vector is orthogonal to booth hyperplanes let $\\mathbf{z_0} \\in H_{-1}$  and construct the vector\n",
    "\n",
    "<img src='static/directors.png' width=\"40%\" height=\"40%\">\n",
    "\n",
    "$$\\mathbf{z_1}  = \\mathbf{z_0}+m \\frac{\\mathbf{w}}{||\\mathbf{w}||}$$\n",
    "\n",
    "\n",
    "This vector lies on $H_1$ then\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{w'z_1}+b &= 1 \\\\\n",
    "\\mathbf{w'}\\left( \\mathbf{z_0}+m \\frac{\\mathbf{w}}{||\\mathbf{w}||} \\right)+b &= 1 \\\\\n",
    "\\mathbf{w'}\\mathbf{z_0}+b+m \\frac{\\mathbf{w'}\\mathbf{w}}{||\\mathbf{w}||}&= 1 \\\\\n",
    "-1+m \\frac{||\\mathbf{w}||^2}{||\\mathbf{w}||}+b &= 1\\\\\n",
    "m||\\mathbf{w}|| &= 2 \\\\\n",
    "m &= \\frac{2}{||\\mathbf{w}||} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "Since $0 < \\frac{2}{||\\mathbf{w}||} $ implies there is another parallel hyperplane  $H=\\{\\mathbf{x} \\in \\mathbb{R}^p : w_1 x_1 + \\dots +w_p x_p + b = 0 \\}$ such that the distance between that hyperplane to the others is exactly $\\frac{1}{||\\mathbf{w}||}$.  \n",
    "\n",
    "**Definition**: The set of datapoints that lies on $H_1$ or $H_{-1}$ are called the support vectors of H.\n",
    "\n",
    "$$SV(H) = \\{ x_i \\in \\mathbb{R}^p : \\mathbf{w'x_i}+b=1 \\lor \\mathbf{w'x_i}=-1\\}$$\n",
    "\n",
    "<img src='static/supportVectors.png' width=\"40%\" height=\"40%\">\n",
    "\n",
    "This means that maximizing the margin is the same that minimizing the norm of the weights. Note that the discriminant classify correctly a vector when $y_i (\\mathbf{w'x_i}+b) \\geq 1 $\n",
    "\n",
    "So the primal problem consists in to:\n",
    "\n",
    "$$\n",
    "Max_{\\mathbf{w},b} \\left \\{ \\frac{2}{||\\mathbf{w}||} \\right \\}\\\\ \n",
    "s.t. y_i (\\mathbf{w'x_i}+b) \\geq 1 \n",
    "$$\n",
    "\n",
    "The dual formulation for the optimization problem is:\n",
    "\n",
    "$$\n",
    "Min_{\\mathbf{w},b} \\left \\{ \\frac{||\\mathbf{w}||^2}{2} \\right \\}\\\\ \n",
    "s.t. y_i (\\mathbf{w'x_i}+b) \\geq 1 \n",
    "$$\n",
    "\n",
    "This is a constrained optimization problem it can be solved by the **Lagrangian multipler** method because it is quadratic, the surface is a hyperparaboloid, with just a single global minimum. in general the Lagraninan objetive functions its given by:\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{x,\\alpha})=f(\\mathbf{x})-\\sum_{i=1}^n \\alpha_i g_i (\\mathbf{x})$$\n",
    "\n",
    "In this case $f(\\mathbf{w},b):=\\frac{||\\mathbf{w}||^2}{2}$ and $g_i(\\mathbf{w},\\alpha):= \\alpha_i (y_i (\\mathbf{w'x_i}+b)-1)$ st. $\\alpha_i \\geq 0$, so the Lagraninan objetive function is:\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{w},b,\\mathbf{\\alpha})= \\frac{\\mathbf{w'w}}{2}-\\sum_{i=1}^n \\alpha_i (y_i (\\mathbf{w'x_i}+b)-1)$$\n",
    "\n",
    "The problem consist into:\n",
    "\n",
    "$$\n",
    "Min_{\\mathbf{w},b} \\left \\{ Max_{\\alpha_i \\geq 0} \\left \\{ \\frac{\\mathbf{w'w}}{2}-\\sum_{i=1}^n \\alpha_i (y_i (\\mathbf{w'x_i}+b)-1) \\right \\} \\right \\}\\\\ \n",
    "$$\n",
    "\n",
    "Slater’s condition from convex optimization guarantees that its possible swap the Min and Max problems:\n",
    "\n",
    "$$\n",
    "Min_{\\mathbf{w},b} \\left \\{ Max_{\\alpha_i \\geq 0} \\left \\{ \\frac{\\mathbf{w'w}}{2}-\\sum_{i=1}^n \\alpha_i (y_i (\\mathbf{w'x_i}+b)-1) \\right \\} \\right \\} = Max_{\\alpha_i \\geq 0} \\left \\{ Min_{\\mathbf{w},b} \\left \\{ \\frac{\\mathbf{w'w}}{2}-\\sum_{i=1}^n \\alpha_i (y_i (\\mathbf{w'x_i}+b)-1) \\right \\} \\right \\}\\\\ \n",
    "$$\n",
    "\n",
    "This implies is possible solve for optimal $\\mathbf{w}$, b and then put it as function of $\\mathbf{\\alpha}$. Computing the partial derivates:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_j} &=  \\frac{\\partial}{\\partial w_j} \\left( \\frac{\\mathbf{w'w}}{2}-\\sum_{i=1}^n \\alpha_i (y_i (\\mathbf{w'x_i}+b)-1)  \\right) = 0\\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b} &=  \\frac{\\partial}{\\partial b} \\left( \\frac{\\mathbf{w'w}}{2}-\\sum_{i=1}^n \\alpha_i (y_i (\\mathbf{w'x_i}+b)-1)  \\right) = 0\\\\\n",
    "\\end{align*}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Computing the partial derivates for some entrie $w_j$ of $\\mathbf{w}$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial w_j} \\left( \\frac{\\mathbf{w'w}}{2}-\\sum_{i=1}^n \\alpha_i \\left(y_i \\left(\\mathbf{w'x_i}+b\\right)-1\\right)  \\right) &= \\frac{\\partial}{\\partial w_j} \\left( \\frac{1}{2} \\sum_{k=1}^p w_i^2 -\\sum_{i=1}^n \\alpha_i \\left(y_i \\left(\\sum_{k=1}^p w_k x_{ik} +b\\right)-1\\right)  \\right) \\\\\n",
    "&= \\frac{\\partial}{\\partial w_j} \\left( \\frac{1}{2} \\sum_{k=1}^p w_i^2  -\\sum_{i=1}^n \\sum_{k=1}^p \\alpha_i y_i w_k x_{ik}-\\sum_{i=1}^n \\alpha_i y_i b + \\sum_{i=1}^n \\alpha_i \\right) \\\\\n",
    "&= \\frac{1}{2} \\sum_{k=1}^p \\frac{\\partial}{\\partial w_j} \\left( w_i^2  \\right) -\\sum_{i=1}^n \\sum_{k=1}^p \\frac{\\partial}{\\partial w_j} ( \\alpha_i y_i w_k x_{ik})-\\sum_{i=1}^n \\frac{\\partial}{\\partial w_j} ( \\alpha_i y_i b ) + \\sum_{i=1}^n \\frac{\\partial}{\\partial w_j} ( \\alpha_i ) \\\\\n",
    "&= w_j -\\sum_{i=1}^n  \\alpha_i y_i x_j\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Solving for $w_j$\n",
    "\n",
    "$$\n",
    "w_j -\\sum_{i=1}^n  \\alpha_i y_i x_j = 0 \\implies w_j = \\sum_{i=1}^n  \\alpha_i y_i x_j \\implies \\mathbf{w} = \\sum_{i=1}^n  \\alpha_i y_i \\mathbf{x_i}\n",
    "$$\n",
    "\n",
    "Computing the partial derivates for the intercepto $b$:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial b} \\left( \\frac{\\mathbf{w'w}}{2}-\\sum_{i=1}^n \\alpha_i \\left(y_i \\left(\\mathbf{w'x_i}+b\\right)-1\\right)  \\right) &= \\frac{1}{2} \\sum_{k=1}^p \\frac{\\partial}{\\partial b} \\left( w_i^2  \\right) -\\sum_{i=1}^n \\sum_{k=1}^p \\frac{\\partial}{\\partial b} ( \\alpha_i y_i w_k x_{ik})-\\sum_{i=1}^n \\frac{\\partial}{\\partial b} ( \\alpha_i y_i b ) + \\sum_{i=1}^n \\frac{\\partial}{\\partial b} ( \\alpha_i ) \\\\\n",
    "&= -\\sum_{i=1}^n  \\alpha_i y_i \\\\\n",
    "\\end{align*} \\\\\n",
    "\\implies -\\sum_{i=1}^n  \\alpha_i y_i  = 0 \\implies \\sum_{i=1}^n  \\alpha_i y_i  = 0\n",
    "$$\n",
    "\n",
    "By substituting for $\\mathbf{w}$ and $b$ back in the original equations its possible get rid of the dependence on $\\mathbf{w}$ and $b$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\mathbf{w},b,\\mathbf{\\alpha})&=\\frac{||\\mathbf{w}||^2}{2}-\\sum_{i=1}^n \\alpha_i y_i (\\mathbf{w'x_i}+b)+\\sum_{i=1}^n \\alpha_i\\\\\n",
    "&= \\frac{1}{2} \\langle  \\sum_{i=1}^n  \\alpha_i y_i \\mathbf{x_i} , \\sum_{j=1}^n  \\alpha_j y_j \\mathbf{x_j} \\rangle -\\sum_{i=1}^n \\alpha_i y_i \\langle \\mathbf{w,x_i} \\rangle + \\sum_{i=1}^n \\alpha_i y_i b +\\sum_{i=1}^n \\alpha_i\\\\\n",
    "&= \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n  \\langle    \\alpha_i y_i \\mathbf{x_i} ,  \\alpha_j y_j \\mathbf{x_j} \\rangle -\\sum_{i=1}^n \\alpha_i y_i \\langle \\sum_{i=j}^n  \\alpha_j y_j \\mathbf{x_j}, \\mathbf{x_i} \\rangle + b \\sum_{i=1}^n \\alpha_i y_i  +\\sum_{i=1}^n \\alpha_i\\\\\n",
    "&= \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\langle \\alpha_i y_i \\mathbf{x_i}, \\alpha_j y_j \\mathbf{x_j} \\rangle -\\sum_{i=1}^n \\sum_{i=j}^n \\alpha_i y_i \\langle  \\alpha_j y_j \\mathbf{x_j}, \\mathbf{x_i} \\rangle +\\sum_{i=1}^n \\alpha_i \\\\\n",
    "&= \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\langle \\alpha_i y_i \\mathbf{x_i}, \\alpha_j y_j \\mathbf{x_j} \\rangle -\\sum_{i=1}^n \\sum_{i=j}^n \\langle  \\alpha_j y_j \\mathbf{x_j}, \\alpha_i y_i \\mathbf{x_i} \\rangle +\\sum_{i=1}^n\n",
    "\\alpha_i\\\\\n",
    "\\mathcal{L}(\\mathbf{\\alpha}) &= - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\langle \\alpha_i y_i \\mathbf{x_i}, \\alpha_j y_j \\mathbf{x_j} \\rangle +\\sum_{i=1}^n \\alpha_i\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Max_{\\alpha_i \\geq 0} \\left \\{ \\mathcal{L}(\\mathbf{\\alpha}) = - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\langle \\alpha_i y_i \\mathbf{x_i}, \\alpha_j y_j \\mathbf{x_j} \\rangle +\\sum_{i=1}^n\\alpha_i \\right \\}\\\\ \n",
    "$$\n",
    "\n",
    "Lets $\\mathbf{\\alpha^*}=(\\alpha_1^*,...,\\alpha_n^*)$ the critical points for $\\mathcal{L}(\\mathbf{\\alpha})$ and recalling from the original discriminant $y_i = \\langle \\mathbf{x_i,w}\\rangle +b$, the critical $\\mathbf{w^*},b^*$ are:\n",
    "\n",
    "$$\n",
    "\\mathbf{w^*} = \\sum_{i=0}^n \\alpha_i^* y_i \\mathbf{x_i} \\quad b^* = y_k - \\langle \\mathbf{w,x_k} \\rangle = y_k - \\langle \\sum_{i=0}^n \\alpha_i^* y_i \\mathbf{x_i},\\mathbf{x_k} \\rangle\n",
    "$$\n",
    "\n",
    "For some $\\mathbf{x_k} \\in SV(H)$ and their associated label $y_k$. Now, replacing the critical values in the original discriminant equation, the estimated $\\hat{y_i}$ its given by:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{y_i} &= \\langle \\mathbf{x,w^*}\\rangle +b^* \\\\ \n",
    "&= \\langle \\mathbf{x},\\sum_{i=0}^n \\alpha_i^* y_i x_i\\rangle+y_k - \\langle \\mathbf{w,x_k} \\rangle\\\\\n",
    "&= \\langle \\mathbf{x}, \\sum_{i=0}^n \\alpha_i^* y_i \\mathbf{x_i} \\rangle+y_k - \\langle \\sum_{i=0}^n \\alpha_i^* y_i \\mathbf{x_i},\\mathbf{x_k} \\rangle \\\\\n",
    "\\hat{y_i}(\\mathbf{x}) &= \\sum_{i=0}^n \\alpha_i^* y_i \\langle \\mathbf{x}, \\mathbf{x_i} \\rangle+y_k - \\sum_{i=0}^n \\alpha_i^* y_i \\langle \\mathbf{x_i},\\mathbf{x_k} \\rangle \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Note that this estimator, derives a natural desicion rule for some $\\mathbf{x} \\in \\mathbb{R^p}$ :\n",
    "\n",
    "$$\n",
    "Label(\\mathbf{x}) = \n",
    "\\begin{cases}\n",
    "\\begin{align*}\n",
    "1 \\quad if \\quad  sign(\\hat{y_i}(\\mathbf{x})) > 0 \\\\\n",
    "-1 \\quad  if \\quad  sign(\\hat{y_i}(\\mathbf{x})) < 0\\\\\n",
    "\\end{align*}\n",
    "\\end{cases}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

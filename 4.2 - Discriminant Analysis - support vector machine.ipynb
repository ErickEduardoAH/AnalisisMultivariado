{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Support Vector Machine\n",
    "\n",
    "---\n",
    "> Erick Eduardo Aguilar Hernández:\n",
    "> * mat.ErickAguilar@ciencias.unam.mx\n",
    "> * isc.ErickAguilar@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suposse for the logistic regression model that given a fixed probability $p_0$ its necessary idenfy the regions in the feature space $p(\\mathbf{x};\\mathbf{\\beta}) > p_0$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p_0 &= \\frac{1}{1+e^{-(\\beta_0+\\beta_1 x_1 + \\dots + \\beta_p x_p)}} \\\\\n",
    "ln\\left(\\frac{ p_0}{1-p_0}\\right) & = \\beta_0+\\beta_1 x_1 + \\dots + \\beta_p x_p \\\\\n",
    "0 & = - ln\\left(\\frac{ p_0}{1-p_0}\\right) + \\beta_0 +\\beta_1 x_1 + \\dots + \\beta_p x_p\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Then the logistic model splits the feature space in two regions.\n",
    "\n",
    "$$\n",
    "P_{p_0} = \\{ \\mathbf{x} \\in \\mathbb{R}^p : - ln\\left(\\frac{ p_0}{1-p_0}\\right) + \\beta_0 +\\beta_1 x_1 + \\dots + \\beta_p x_p \\geq 0 \\} \\\\\n",
    "N_{p_0} =\\{ \\mathbf{x} \\in \\mathbb{R}^p : - ln\\left(\\frac{ p_0}{1-p_0}\\right) + \\beta_0 +\\beta_1 x_1 + \\dots + \\beta_p x_p < 0 \\}\n",
    "$$\n",
    "\n",
    "Particularly for $p_0$= 0.5 the regions are:\n",
    "\n",
    "$$P_{0.5} = \\{ \\mathbf{x} \\in \\mathbb{R}^p : \\beta_0 +\\beta_1 x_1 + \\dots + \\beta_p x_p \\geq 0 \\} \\\\\n",
    "N_{0.5} =\\{ \\mathbf{x} \\in \\mathbb{R}^p : \\beta_0 +\\beta_1 x_1 + \\dots + \\beta_p x_p < 0 \\}\n",
    "$$\n",
    "\n",
    "Although the logistic regression folds the feature space in to many different regions delimited by a decision boundary as hyperplanes, the logistic model dont provides the best threshold in order to find the best hyperplane that separates the populations. This motivates a new discriminant method known as suppor vector machine who principal objetive its try to determine the best hyperplane separator.\n",
    "\n",
    "\n",
    "### Geometric margins\n",
    "___\n",
    "\n",
    "Lets the hyperplane H with equation $w_1 x_1 + \\dots +w_p x_p + b = \\mathbf{w'x}+b$. Consider a point $\\mathbf{x} \\in H$ and other $\\mathbf{x_i} \\in \\mathbb{R}^p$ with label $y \\in \\{1,-1\\}$ (1 if $\\mathbf{x}$ belongs to positive class, -1 if $\\mathbf{x}$ belongs to negative class) then the distance between the point $\\mathbf{x_i}$ and H its given by:\n",
    "\n",
    "<img src='static/proyection.png' width=\"40%\" height=\"40%\">\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "d(\\mathbf{x_i},H) &= || Proy_\\mathbf{w} (\\mathbf{x}-\\mathbf{x_i}) || \\\\\n",
    " &= \\frac{< \\mathbf{w},(\\mathbf{x}-\\mathbf{x_i})>}{||\\mathbf{w}||} \\\\\n",
    " &= \\frac{w_1(x_{i1}-x_1)+\\dots+w_p(x_{ip}-xp)}{||\\mathbf{w}||} \\\\\n",
    " &= \\frac{w_1x_{i1}-w_1x_1+\\dots+w_px_{ip}-w_px_p}{||\\mathbf{w}||} \\\\\n",
    " &= \\frac{w_1x_{i1}+\\dots+w_px_{ip}-w_1x_1-\\dots-w_px_p}{||\\mathbf{w}||} \\\\\n",
    "&= \\frac{w_1x_{i1}+\\dots+w_px_{ip}+b}{||\\mathbf{w}||} \\\\\n",
    "d(\\mathbf{x_i},H)  &=\\frac{<\\mathbf{w},\\mathbf{x_i}>+b}{||\\mathbf{w}||}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Lets deffine the geometric margin for the ith observation $\\gamma_i$ as the oriented distance form the observation to the hyperplane such that if $\\mathbf{x_i}$ its a positive obervation then the orginal distance its conserved but if $\\mathbf{x_i}$ its a negative obervation then the distance have to change the sign to $-d(\\mathbf{x_i},H)$. since $y_i$ contains this information the equation for gamma its given by:\n",
    "\n",
    "<img src='static/margins.png' width=\"40%\" height=\"40%\">\n",
    "\n",
    "$$\\gamma_{\\mathbf{x_i}} (\\mathbf{w})= y_i \\left(\\frac{<\\mathbf{w},\\mathbf{x_i}>+b}{||\\mathbf{w}||}\\right) = y_i \\left(\\left(\\frac{\\mathbf{w'}}{||\\mathbf{w}||}\\right)\\mathbf{x_i} + \\frac{b}{||\\mathbf{w}||} \\right)$$\n",
    "\n",
    "\n",
    "### Optimal margin discriminant\n",
    "___\n",
    "\n",
    "The optimal hyperplane its given by the $\\mathbf{w} \\in \\mathbb{R}^p$ and $b \\in \\mathbb{R}$ such that:\n",
    "\n",
    "$$\n",
    "Max_w \\left \\{ Min_{\\mathbf{x_i}} \\left \\{ \\left(\\frac{\\mathbf{w'}}{||\\mathbf{w}||}\\right)\\mathbf{x_i} + \\frac{b}{||\\mathbf{w}||} \\right \\} \\right \\} \\\\\n",
    "s.t. sign(\\gamma_{\\mathbf{x_i}})=y_i\n",
    "$$\n",
    "\n",
    "Solve this problemm in general its complicated due to:\n",
    "\n",
    "* The objective is nonlinear in two ways: the absolute value and the projection requires you to take a norm and * divide.\n",
    "* The constraints are nonlinear due to the sign comparisons.\n",
    "* There’s a min and a max! A priori, we have to do this because we don’t know which point is going to be the closest to the hyperplane.\n",
    "\n",
    "**Reformulating the problem**\n",
    "\n",
    "Consider 2 parallel hyperplanes $H_{1}$ and $H_{-1}$ with equations $w_1 x_1 + \\dots +w_p x_p + b = 1$ and $w_1 x_1 + \\dots +w_p x_p + b = -1$ conrrespondly and suposse there is not data observations between that planes.\n",
    "\n",
    "**Definition**: If $\\mathbf{z_0} \\in H_{-1}$, the margin is the perpendicular distance between $\\mathbf{z_0}$ and $H_1$ i.e. $m=d(\\mathbf{z_0},H_1)$.\n",
    "\n",
    "Contruct the following vector in same direction of $\\mathbf{w}$ but with lenght m, $\\mathbf{u} = m \\frac{\\mathbf{w}}{||\\mathbf{w}||}$, this vector is orthogonal to booth hyperplanes let $\\mathbf{z_0} \\in H_{-1}$  and construct the vector\n",
    "\n",
    "<img src='static/directors.png' width=\"40%\" height=\"40%\">\n",
    "\n",
    "$$\\mathbf{z_1}  = \\mathbf{z_0}+m \\frac{\\mathbf{w}}{||\\mathbf{w}||}$$\n",
    "\n",
    "\n",
    "This vector lies on $H_1$ then\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{w'z_1}+b &= 1 \\\\\n",
    "\\mathbf{w'}\\left( \\mathbf{z_0}+m \\frac{\\mathbf{w}}{||\\mathbf{w}||} \\right)+b &= 1 \\\\\n",
    "\\mathbf{w'}\\mathbf{z_0}+b+m \\frac{\\mathbf{w'}\\mathbf{w}}{||\\mathbf{w}||}&= 1 \\\\\n",
    "-1+m \\frac{||\\mathbf{w}||^2}{||\\mathbf{w}||}+b &= 1\\\\\n",
    "m||\\mathbf{w}|| &= 2 \\\\\n",
    "m &= \\frac{2}{||\\mathbf{w}||} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "Since $0 < \\frac{2}{||\\mathbf{w}||} $ implies there is another parallel hyperplane  $H = \\{ \\mathbf{x} \\in \\mathbb{R}^p : w_1 x_1 + \\dots +w_p x_p + b = 0 \\}$ such that the distance between that hyperplane to the others is exactly $\\frac{1}{||\\mathbf{w}||}$.\n",
    "\n",
    "**Definition**: The set of datapoints that lies on $H_1$ or $H_{-1}$ are called the support vectors of H.\n",
    "\n",
    "$$SV(H) = \\{ x_i \\in \\mathbb{R}^p : \\mathbf{w'x_i}+b=1 \\lor \\mathbf{w'x_i}=-1\\}$$\n",
    "\n",
    "<img src='static/supportVectors.png' width=\"40%\" height=\"40%\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
